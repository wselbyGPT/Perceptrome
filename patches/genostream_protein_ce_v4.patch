Binary files genostream/__pycache__/__init__.cpython-311.pyc and ../work_genostream/genostream/__pycache__/__init__.cpython-311.pyc differ
Binary files genostream/__pycache__/cli.cpython-311.pyc and ../work_genostream/genostream/__pycache__/cli.cpython-311.pyc differ
Binary files genostream/__pycache__/config.cpython-311.pyc and ../work_genostream/genostream/__pycache__/config.cpython-311.pyc differ
Binary files genostream/__pycache__/encoding.cpython-311.pyc and ../work_genostream/genostream/__pycache__/encoding.cpython-311.pyc differ
Binary files genostream/__pycache__/generate.cpython-311.pyc and ../work_genostream/genostream/__pycache__/generate.cpython-311.pyc differ
Binary files genostream/__pycache__/io_utils.cpython-311.pyc and ../work_genostream/genostream/__pycache__/io_utils.cpython-311.pyc differ
Binary files genostream/__pycache__/model.cpython-311.pyc and ../work_genostream/genostream/__pycache__/model.cpython-311.pyc differ
Binary files genostream/__pycache__/ncbi_fetch.cpython-311.pyc and ../work_genostream/genostream/__pycache__/ncbi_fetch.cpython-311.pyc differ
Binary files genostream/__pycache__/scope.cpython-311.pyc and ../work_genostream/genostream/__pycache__/scope.cpython-311.pyc differ
Binary files genostream/__pycache__/scrope.cpython-311.pyc and ../work_genostream/genostream/__pycache__/scrope.cpython-311.pyc differ
Binary files genostream/__pycache__/training.cpython-311.pyc and ../work_genostream/genostream/__pycache__/training.cpython-311.pyc differ
diff -ruN genostream/cli.py ../work_genostream/genostream/cli.py
--- genostream/cli.py	2025-12-13 06:06:44.422560148 +0000
+++ ../work_genostream/genostream/cli.py	2025-12-13 06:08:28.648453416 +0000
@@ -178,6 +178,8 @@
         steps=steps, batch_size=batch_size,
         state=state, io_cfg=io_cfg, train_cfg=train_cfg,
         tokenizer=tok, window_size_bp=window_size,
+        loss_type=getattr(args, "loss_type", None),
+        mask_prob=getattr(args, "mask_prob", None),
     )
 
     pvc = state["plasmid_visit_counts"]
@@ -219,7 +221,15 @@
             save_to_disk=True, out_path=enc_path
         )
 
-    errors = compute_window_errors(args.accession, encoded, io_cfg=io_cfg, train_cfg=train_cfg, tokenizer=tok, window_size_bp=window_size)
+    errors = compute_window_errors(
+        args.accession,
+        encoded,
+        io_cfg=io_cfg,
+        train_cfg=train_cfg,
+        tokenizer=tok,
+        window_size_bp=window_size,
+        loss_type=getattr(args, "loss_type", None),
+    )
     metric = compute_gc_from_encoded(encoded, tokenizer=tok)
 
     curses.wrapper(
@@ -279,10 +289,12 @@
     seq_len, vocab_size = tokenizer_meta(tok, window_size)
     hidden_dim = train_cfg.hidden_dim
 
+    lt = (args.loss_type if getattr(args, "loss_type", None) is not None else ("ce" if tok == "aa" else "mse"))
+
     model, optimizer, global_step, ckpt_path = load_or_init_model(
         io_cfg=io_cfg, seq_len=seq_len, vocab_size=vocab_size,
         hidden_dim=hidden_dim, learning_rate=train_cfg.learning_rate,
-        device=device, tokenizer=tok
+        device=device, tokenizer=tok, loss_type=lt
     )
 
     windows_tensor = torch.from_numpy(encoded)
@@ -296,6 +308,7 @@
         steps_target=steps, steps_done=0,
         beta_kl=train_cfg.beta_kl, kl_warmup_steps=train_cfg.kl_warmup_steps,
         max_grad_norm=train_cfg.max_grad_norm,
+        loss_type=lt, seq_len=int(seq_len), vocab_size=int(vocab_size),
     )
 
     curses.wrapper(
@@ -365,6 +378,8 @@
                 steps=steps_per_plasmid, batch_size=batch_size,
                 state=state, io_cfg=io_cfg, train_cfg=train_cfg,
                 tokenizer=tok, window_size_bp=window_size,
+                loss_type=getattr(args, "loss_type", None),
+                mask_prob=getattr(args, "mask_prob", None),
             )
 
             pvc = state["plasmid_visit_counts"]
@@ -431,6 +446,10 @@
         temperature=args.temperature,
         name=args.name,
         output_path=args.output,
+        reject=bool(getattr(args, "reject", False)),
+        reject_tries=int(getattr(args, "reject_tries", 40)),
+        reject_max_run=int(getattr(args, "reject_max_run", 10)),
+        reject_max_x_frac=float(getattr(args, "reject_max_x_frac", 0.15)),
     )
     print(f"[generate-protein] wrote {len(seq)} aa -> {args.output}")
     return 0
@@ -454,6 +473,20 @@
         sp.add_argument("--min-orf-aa", type=int, default=None, help="AA tokenizer: minimum ORF length in amino acids (default from config)")
         sp.add_argument("--source", choices=["fasta","genbank"], default=None, help="Sequence record source. Default: fasta for base/codon, genbank for aa.")
 
+    def add_loss_args(sp):
+        sp.add_argument(
+            "--loss-type",
+            choices=["mse", "ce"],
+            default=None,
+            help="Override reconstruction loss. Default: ce for aa, mse for base/codon.",
+        )
+        sp.add_argument(
+            "--mask-prob",
+            type=float,
+            default=None,
+            help="Denoising mask probability (AA tokenizer only). Default: 0.05 for aa, 0 for others.",
+        )
+
     s = sub.add_parser("encode-one")
     s.add_argument("accession")
     s.add_argument("--window-size", type=int, default=None)
@@ -469,6 +502,7 @@
     s.add_argument("--stride", type=int, default=None)
     s.add_argument("--reencode", action="store_true")
     add_tok_args(s)
+    add_loss_args(s)
     s.set_defaults(func=cmd_train_one)
 
     s = sub.add_parser("scope-one")
@@ -478,6 +512,7 @@
     s.add_argument("--fps", type=float, default=12.0)
     s.add_argument("--reencode", action="store_true")
     add_tok_args(s)
+    s.add_argument("--loss-type", choices=["mse", "ce"], default=None, help="Override loss used for error metric (default: ce for aa, mse for base/codon)")
     s.set_defaults(func=cmd_scope_one)
 
     s = sub.add_parser("scope-stream")
@@ -490,6 +525,7 @@
     s.add_argument("--update-every", type=int, default=5)
     s.add_argument("--reencode", action="store_true")
     add_tok_args(s)
+    add_loss_args(s)
     s.set_defaults(func=cmd_scope_stream)
 
     s = sub.add_parser("stream")
@@ -501,6 +537,7 @@
     s.add_argument("--stride", type=int, default=None)
     s.add_argument("--delete-cache", action="store_true")
     add_tok_args(s)
+    add_loss_args(s)
     s.set_defaults(func=cmd_stream)
 
     s = sub.add_parser("generate-plasmid")
@@ -525,6 +562,10 @@
     s.add_argument("--seed", type=int, default=None)
     s.add_argument("--latent-scale", type=float, default=1.0)
     s.add_argument("--temperature", type=float, default=1.0)
+    s.add_argument("--reject", action="store_true", help="Rejection-sample until a basic protein-like filter passes")
+    s.add_argument("--reject-tries", type=int, default=40)
+    s.add_argument("--reject-max-run", type=int, default=10, help="Reject if any AA repeats longer than this")
+    s.add_argument("--reject-max-x-frac", type=float, default=0.15, help="Reject if fraction of 'X' exceeds this")
     s.set_defaults(func=cmd_generate_protein)
 
     return p
diff -ruN genostream/generate.py ../work_genostream/genostream/generate.py
--- genostream/generate.py	2025-12-13 04:00:38.000000000 +0000
+++ ../work_genostream/genostream/generate.py	2025-12-13 06:06:24.105799453 +0000
@@ -12,18 +12,41 @@
 from .model import get_device, load_or_init_model
 from .encoding import tokenizer_meta, IDX_TO_CODON, CODON_VOCAB_SIZE, GC_COUNT_PER_TOKEN, IDX_TO_AA, AA_VOCAB_SIZE
 
-def _sample_index(weights: np.ndarray, temperature: float) -> int:
-    w = weights.astype(np.float64)
-    w = np.clip(w, 1e-9, None)
-    temperature = max(1e-3, float(temperature))
-    if temperature != 1.0:
-        w = w ** (1.0 / temperature)
-    s = w.sum()
-    if s <= 0:
+def _sample_from_logits(logits: np.ndarray, temperature: float) -> int:
+    """Sample an index from a logits vector using softmax( logits / T )."""
+    x = logits.astype(np.float64)
+    T = max(1e-3, float(temperature))
+    x = x / T
+    x = x - np.max(x)
+    w = np.exp(x)
+    s = float(w.sum())
+    if not np.isfinite(s) or s <= 0:
         return int(np.random.randint(0, w.shape[0]))
     w /= s
     return int(np.random.choice(w.shape[0], p=w))
 
+
+def _passes_protein_filters(seq: str, max_run: int, max_x_frac: float) -> bool:
+    if not seq:
+        return False
+    if max_x_frac is not None and max_x_frac >= 0:
+        xf = seq.count("X") / float(len(seq))
+        if xf > float(max_x_frac):
+            return False
+    if max_run is not None and max_run > 0:
+        run = 1
+        best = 1
+        for i in range(1, len(seq)):
+            if seq[i] == seq[i-1]:
+                run += 1
+                if run > best:
+                    best = run
+            else:
+                run = 1
+        if best > int(max_run):
+            return False
+    return True
+
 def generate_plasmid_sequence(
     train_cfg: TrainingConfig,
     io_cfg: IOConfig,
@@ -64,6 +87,7 @@
         learning_rate=train_cfg.learning_rate,
         device=device,
         tokenizer=tok,
+        loss_type="mse",
     )
     model.eval()
 
@@ -83,25 +107,27 @@
     with torch.no_grad():
         for _ in range(n_windows):
             z = torch.randn(1, hidden_dim, device=device) * latent_scale
-            recon_flat = model.decode(z)   # (1, seq_len*vocab)
-            recon = recon_flat.view(seq_len, vocab_size).cpu().numpy()
+            logits_flat = model.decode(z)   # (1, seq_len*vocab)
+            logits = logits_flat.view(seq_len, vocab_size).cpu().numpy()
 
             if tok == "base":
                 idx_to_base = ["A", "C", "G", "T"]
                 for j in range(seq_len):
-                    w = recon[j].copy()
+                    # base/codon models are trained with MSE on sigmoid weights
+                    w = 1.0 / (1.0 + np.exp(-logits[j]))
                     # apply gc bias to C/G
                     if gc_bias != 1.0:
                         w[1] *= gc_bias
                         w[2] *= gc_bias
-                    idx = _sample_index(w, temperature)
+                    # weights, not logits: convert to pseudo-logits via log
+                    idx = _sample_from_logits(np.log(np.clip(w, 1e-9, None)), temperature)
                     seq_parts.append(idx_to_base[idx])
             else:
                 for j in range(seq_len):
-                    w = recon[j].copy()
+                    w = 1.0 / (1.0 + np.exp(-logits[j]))
                     if gc_bias != 1.0:
                         w *= (gc_bias ** GC_COUNT_PER_TOKEN[: w.shape[0]])
-                    idx = _sample_index(w, temperature)
+                    idx = _sample_from_logits(np.log(np.clip(w, 1e-9, None)), temperature)
                     seq_parts.append(IDX_TO_CODON[idx])
 
     seq = "".join(seq_parts)
@@ -127,6 +153,10 @@
     temperature: float,
     name: str,
     output_path: str,
+    reject: bool = False,
+    reject_tries: int = 40,
+    reject_max_run: int = 10,
+    reject_max_x_frac: float = 0.15,
 ) -> str:
     if torch is None:
         raise RuntimeError("PyTorch not installed.")
@@ -150,6 +180,7 @@
         learning_rate=train_cfg.learning_rate,
         device=device,
         tokenizer=tok,
+        loss_type="ce",
     )
     model.eval()
 
@@ -163,19 +194,31 @@
     temperature = float(temperature)
     latent_scale = float(latent_scale)
 
-    aa_chars: List[str] = []
-
-    with torch.no_grad():
-        for _ in range(n_windows):
-            z = torch.randn(1, hidden_dim, device=device) * latent_scale
-            recon_flat = model.decode(z)
-            recon = recon_flat.view(seq_len, vocab_size).cpu().numpy()
-
-            for j in range(seq_len):
-                idx = _sample_index(recon[j], temperature)
-                aa_chars.append(IDX_TO_AA[idx])
-
-    seq = "".join(aa_chars)[:target_aa]
+    def _sample_once() -> str:
+        aa_chars: List[str] = []
+        with torch.no_grad():
+            for _ in range(n_windows):
+                z = torch.randn(1, hidden_dim, device=device) * latent_scale
+                logits_flat = model.decode(z)
+                logits = logits_flat.view(seq_len, vocab_size).cpu().numpy()
+                for j in range(seq_len):
+                    idx = _sample_from_logits(logits[j], temperature)
+                    aa_chars.append(IDX_TO_AA[idx])
+        return "".join(aa_chars)[:target_aa]
+
+    if reject:
+        tries = max(1, int(reject_tries))
+        for t in range(tries):
+            seq = _sample_once()
+            if _passes_protein_filters(seq, max_run=int(reject_max_run), max_x_frac=float(reject_max_x_frac)):
+                break
+            if (t + 1) % 10 == 0:
+                logging.info(f"[generate-protein] rejection: {t+1}/{tries} rejected")
+        else:
+            logging.warning("[generate-protein] rejection-sampling exhausted tries; using last sample")
+            seq = _sample_once()
+    else:
+        seq = _sample_once()
 
     out_dir = os.path.dirname(output_path) or "."
     os.makedirs(out_dir, exist_ok=True)
diff -ruN genostream/model.py ../work_genostream/genostream/model.py
--- genostream/model.py	2025-12-13 03:51:11.000000000 +0000
+++ ../work_genostream/genostream/model.py	2025-12-13 06:02:11.404539339 +0000
@@ -4,10 +4,12 @@
 try:
     import torch
     from torch import nn, optim
+    import torch.nn.functional as F
 except ImportError:
     torch = None  # type: ignore
     nn = None     # type: ignore
     optim = None  # type: ignore
+    F = None      # type: ignore
 
 from .config import IOConfig
 
@@ -23,7 +25,6 @@
         self.fc2 = nn.Linear(hidden_dim, hidden_dim)
         self.fc_out = nn.Linear(hidden_dim, input_dim)
         self.act = nn.ReLU()
-        self.out_act = nn.Sigmoid()
 
     def encode(self, x: "torch.Tensor") -> Tuple["torch.Tensor", "torch.Tensor"]:
         h = self.act(self.fc1(x))
@@ -35,14 +36,30 @@
         return mu + eps * std
 
     def decode(self, z: "torch.Tensor") -> "torch.Tensor":
+        """Return *logits* (not probabilities).
+
+        - For MSE-based training, we apply sigmoid to these logits in the loss.
+        - For categorical training (CE), these logits are fed directly to softmax/CE.
+        """
         h = self.act(self.fc2(z))
-        return self.out_act(self.fc_out(h))
+        return self.fc_out(h)
+
+    def decode_probs(self, z: "torch.Tensor", seq_len: int, vocab_size: int, loss_type: str) -> "torch.Tensor":
+        """Return probabilities shaped (B, seq_len, vocab_size)."""
+        if torch is None or F is None:
+            raise RuntimeError("PyTorch is required.")
+        logits = self.decode(z).view(z.size(0), int(seq_len), int(vocab_size))
+        lt = str(loss_type).lower()
+        if lt == "ce":
+            return F.softmax(logits, dim=-1)
+        # mse (legacy): independent sigmoid weights
+        return torch.sigmoid(logits)
 
     def forward(self, x: "torch.Tensor") -> Tuple["torch.Tensor", "torch.Tensor", "torch.Tensor"]:
         mu, logvar = self.encode(x)
         z = self.reparameterize(mu, logvar)
-        recon = self.decode(z)
-        return recon, mu, logvar
+        recon_logits = self.decode(z)
+        return recon_logits, mu, logvar
 
 def get_device() -> "torch.device":
     if torch is None:
@@ -57,6 +74,7 @@
     learning_rate: float,
     device: "torch.device",
     tokenizer: str,
+    loss_type: str,
 ) -> Tuple[PlasmidVAE, "optim.Optimizer", int, str]:
     """
     seq_len: number of positions (bp or codons)
@@ -79,6 +97,7 @@
         ck_seq = int(meta.get("seq_len", seq_len))
         ck_vocab = int(meta.get("vocab_size", vocab_size))
         ck_hidden = int(meta.get("hidden_dim", hidden_dim))
+        ck_loss = str(meta.get("loss_type", "mse")).lower()
 
         if ck_tok != tokenizer.lower():
             raise ValueError(f"Checkpoint tokenizer={ck_tok} but requested tokenizer={tokenizer}. Delete {ckpt_path} or match settings.")
@@ -88,6 +107,11 @@
             raise ValueError(f"Checkpoint vocab_size={ck_vocab} but requested vocab_size={vocab_size}. Delete {ckpt_path} or match settings.")
         if ck_hidden != hidden_dim:
             raise ValueError(f"Checkpoint hidden_dim={ck_hidden} but requested hidden_dim={hidden_dim}. Delete {ckpt_path} or match settings.")
+        if ck_loss != str(loss_type).lower():
+            raise ValueError(
+                f"Checkpoint loss_type={ck_loss} but requested loss_type={loss_type}. "
+                f"Delete {ckpt_path} or match settings."
+            )
 
         model.load_state_dict(data["model"])
         optimizer.load_state_dict(data["optim"])
@@ -95,7 +119,10 @@
 
         logging.info(f"Loaded checkpoint {ckpt_path} (tokenizer={ck_tok}, seq_len={ck_seq}, vocab={ck_vocab}, hidden={ck_hidden}, step={global_step})")
     else:
-        logging.info(f"Initializing new VAE (tokenizer={tokenizer}, seq_len={seq_len}, vocab={vocab_size}, input_dim={input_dim}, hidden={hidden_dim}, lr={learning_rate})")
+        logging.info(
+            f"Initializing new VAE (tokenizer={tokenizer}, loss_type={loss_type}, "
+            f"seq_len={seq_len}, vocab={vocab_size}, input_dim={input_dim}, hidden={hidden_dim}, lr={learning_rate})"
+        )
 
     return model, optimizer, global_step, ckpt_path
 
@@ -108,6 +135,7 @@
     seq_len: int,
     vocab_size: int,
     hidden_dim: int,
+    loss_type: str,
 ) -> None:
     if torch is None:
         return
@@ -120,6 +148,7 @@
             "seq_len": int(seq_len),
             "vocab_size": int(vocab_size),
             "hidden_dim": int(hidden_dim),
+            "loss_type": str(loss_type).lower(),
         },
     }
     tmp = ckpt_path + ".tmp"
@@ -128,15 +157,33 @@
     logging.info(f"Saved checkpoint step={global_step} -> {ckpt_path}")
 
 def vae_loss(
-    recon: "torch.Tensor",
+    recon_logits: "torch.Tensor",
     x: "torch.Tensor",
     mu: "torch.Tensor",
     logvar: "torch.Tensor",
     beta_kl: float,
+    loss_type: str,
+    seq_len: int,
+    vocab_size: int,
 ) -> Tuple["torch.Tensor", "torch.Tensor", "torch.Tensor"]:
     if torch is None or nn is None:
         raise RuntimeError("PyTorch required.")
-    mse = nn.MSELoss(reduction="mean")(recon, x)
+
+    lt = str(loss_type).lower()
+    if lt == "ce":
+        if F is None:
+            raise RuntimeError("PyTorch required.")
+        # logits: (B, L*V) -> (B, L, V)
+        logits = recon_logits.view(recon_logits.size(0), int(seq_len), int(vocab_size))
+        targets = x.view(x.size(0), int(seq_len), int(vocab_size)).argmax(dim=2)  # (B, L)
+        # per-position CE, mean over positions and batch
+        ce = F.cross_entropy(logits.view(-1, int(vocab_size)), targets.view(-1), reduction="mean")
+        recon_term = ce
+    else:
+        # legacy: regression on one-hot using sigmoid weights
+        recon = torch.sigmoid(recon_logits)
+        recon_term = nn.MSELoss(reduction="mean")(recon, x)
+
     kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
-    total = mse + float(beta_kl) * kl
-    return total, mse, kl
+    total = recon_term + float(beta_kl) * kl
+    return total, recon_term, kl
diff -ruN genostream/scope.py ../work_genostream/genostream/scope.py
--- genostream/scope.py	2025-12-13 04:09:42.000000000 +0000
+++ ../work_genostream/genostream/scope.py	2025-12-13 06:08:51.468044962 +0000
@@ -141,8 +141,15 @@
     model: PlasmidVAE,
     windows_tensor: "torch.Tensor",
     device: "torch.device",
+    loss_type: str = "mse",
+    seq_len: int = 0,
+    vocab_size: int = 0,
 ) -> np.ndarray:
-    """Compute per-window MSE errors using posterior mean (mu) as z."""
+    """Compute per-window errors using posterior mean (mu) as z.
+
+    - loss_type='mse': MSE between sigmoid(logits) and one-hot input
+    - loss_type='ce': mean cross-entropy (NLL) per window
+    """
     if torch is None:
         raise RuntimeError(
             "PyTorch is not installed. Install it with `pip install torch`."
@@ -156,10 +163,22 @@
         N = wt.size(0)
         x_flat = wt.view(N, -1)
         mu, logvar = model.encode(x_flat)
-        recon_flat = model.decode(mu)
-        recon = recon_flat.view_as(wt)
-        mse = (recon - wt).pow(2).mean(dim=(1, 2))
-        return mse.cpu().numpy().astype(np.float32)
+        logits_flat = model.decode(mu)
+        if str(loss_type).lower() == "ce":
+            import torch.nn.functional as F
+            if seq_len <= 0 or vocab_size <= 0:
+                # Infer shape from tensor
+                seq_len = int(wt.size(1))
+                vocab_size = int(wt.size(2))
+            logits3 = logits_flat.view(N, int(seq_len), int(vocab_size))
+            targets = wt.argmax(dim=2)
+            ce = F.cross_entropy(logits3.view(-1, int(vocab_size)), targets.view(-1), reduction="none")
+            ce_w = ce.view(N, int(seq_len)).mean(dim=1)
+            return ce_w.cpu().numpy().astype(np.float32)
+        else:
+            recon = torch.sigmoid(logits_flat).view_as(wt)
+            mse = (recon - wt).pow(2).mean(dim=(1, 2))
+            return mse.cpu().numpy().astype(np.float32)
 
 
 @dataclass
@@ -176,6 +195,9 @@
     beta_kl: float
     kl_warmup_steps: int
     max_grad_norm: float
+    loss_type: str
+    seq_len: int
+    vocab_size: int
 
 
 def run_scope_stream_ui(
@@ -219,7 +241,8 @@
     paused = False
 
     errors = compute_errors_with_model_and_tensor(
-        ctx.model, windows_tensor, ctx.device
+        ctx.model, windows_tensor, ctx.device,
+        loss_type=ctx.loss_type, seq_len=ctx.seq_len, vocab_size=ctx.vocab_size,
     )
 
     import time
@@ -336,9 +359,10 @@
                     ctx.dataloader_iter = iter(ctx.dataloader)
                     (batch,) = next(ctx.dataloader_iter)
 
-                batch = batch.to(ctx.device)  # (B, W, 4)
+                batch = batch.to(ctx.device)  # (B, L, V)
                 B = batch.size(0)
-                x_flat = batch.view(B, -1)
+                x_target_flat = batch.view(B, -1)
+                x_in_flat = x_target_flat
 
                 # KL annealing
                 if ctx.kl_warmup_steps > 0:
@@ -350,9 +374,10 @@
                     beta = ctx.beta_kl
 
                 ctx.optimizer.zero_grad(set_to_none=True)
-                recon_flat, mu, logvar = ctx.model(x_flat)
+                recon_logits, mu, logvar = ctx.model(x_in_flat)
                 total_loss, recon_loss, kl_loss = vae_loss(
-                    recon_flat, x_flat, mu, logvar, beta
+                    recon_logits, x_target_flat, mu, logvar, beta,
+                    str(ctx.loss_type).lower(), int(ctx.seq_len), int(ctx.vocab_size)
                 )
                 total_loss.backward()
 
@@ -368,7 +393,8 @@
                 ctx.last_total = float(total_loss.item())
 
             errors = compute_errors_with_model_and_tensor(
-                ctx.model, windows_tensor, ctx.device
+                ctx.model, windows_tensor, ctx.device,
+                loss_type=ctx.loss_type, seq_len=ctx.seq_len, vocab_size=ctx.vocab_size,
             )
 
         time.sleep(max(0.0, 1.0 / max(fps, 1e-3)))
diff -ruN genostream/training.py ../work_genostream/genostream/training.py
--- genostream/training.py	2025-12-13 06:06:44.424064088 +0000
+++ ../work_genostream/genostream/training.py	2025-12-13 06:03:42.798524214 +0000
@@ -1,5 +1,5 @@
 import logging, os
-from typing import Dict, Any
+from typing import Dict, Any, Optional
 
 import numpy as np
 
@@ -15,6 +15,32 @@
 from .model import get_device, load_or_init_model, save_checkpoint, vae_loss
 from .encoding import tokenizer_meta
 
+
+def _default_loss_type(tokenizer: str) -> str:
+    # AA/proteome mode benefits strongly from categorical CE.
+    return "ce" if str(tokenizer).lower() == "aa" else "mse"
+
+
+def _apply_aa_mask(batch_onehot: "torch.Tensor", mask_prob: float) -> "torch.Tensor":
+    """Randomly replace some AA positions with X (unknown) in the *input*.
+
+    batch_onehot: (B, L, V) one-hot
+    Returns a modified copy.
+    """
+    if torch is None:
+        raise RuntimeError("PyTorch not installed.")
+    p = float(mask_prob)
+    if p <= 0:
+        return batch_onehot
+    # X is last index in AA vocab (see encoding.AA_VOCAB = 20 + X)
+    X_IDX = batch_onehot.size(2) - 1
+    x = batch_onehot.clone()
+    mask = (torch.rand(x.size(0), x.size(1), device=x.device) < p)
+    if mask.any():
+        x[mask] = 0.0
+        x[mask, X_IDX] = 1.0
+    return x
+
 def train_on_encoded(
     accession: str,
     encoded: np.ndarray,
@@ -25,6 +51,8 @@
     train_cfg: TrainingConfig,
     tokenizer: str,
     window_size_bp: int,
+    loss_type: Optional[str] = None,
+    mask_prob: Optional[float] = None,
 ) -> float:
     if torch is None:
         raise RuntimeError("PyTorch not installed.")
@@ -33,6 +61,9 @@
     seq_len, vocab_size = tokenizer_meta(tokenizer, window_size_bp)
     hidden_dim = train_cfg.hidden_dim
 
+    lt = _default_loss_type(tokenizer) if loss_type is None else str(loss_type).lower()
+    mp = float(mask_prob) if mask_prob is not None else (0.05 if str(tokenizer).lower() == "aa" else 0.0)
+
     model, optimizer, global_step, ckpt_path = load_or_init_model(
         io_cfg=io_cfg,
         seq_len=seq_len,
@@ -41,6 +72,7 @@
         learning_rate=train_cfg.learning_rate,
         device=device,
         tokenizer=tokenizer,
+        loss_type=lt,
     )
 
     windows_tensor = torch.from_numpy(encoded)  # (N, L, V)
@@ -68,7 +100,13 @@
             (batch,) = next(it)
 
         batch = batch.to(device)         # (B, L, V)
-        x_flat = batch.view(batch.size(0), -1)
+        # Denoising (AA only by default): corrupt *input*, keep target clean.
+        if mp > 0 and str(tokenizer).lower() == "aa":
+            x_in = _apply_aa_mask(batch, mp)
+        else:
+            x_in = batch
+        x_flat = x_in.view(x_in.size(0), -1)
+        x_target_flat = batch.view(batch.size(0), -1)
 
         if train_cfg.kl_warmup_steps > 0:
             warm = min(1.0, (global_step + 1) / float(train_cfg.kl_warmup_steps))
@@ -77,8 +115,8 @@
             beta = train_cfg.beta_kl
 
         optimizer.zero_grad(set_to_none=True)
-        recon_flat, mu, logvar = model(x_flat)
-        total, recon, kl = vae_loss(recon_flat, x_flat, mu, logvar, beta)
+        recon_logits, mu, logvar = model(x_flat)
+        total, recon, kl = vae_loss(recon_logits, x_target_flat, mu, logvar, beta, lt, seq_len, vocab_size)
         total.backward()
 
         if train_cfg.max_grad_norm and train_cfg.max_grad_norm > 0:
@@ -92,7 +130,7 @@
 
         if step_count % 10 == 0 or step_count == steps:
             logging.info(
-                f"{accession}: step {step_count}/{steps} total={total.item():.6f} recon={recon.item():.6f} kl={kl.item():.6f} beta={beta:.3g}"
+                f"{accession}: step {step_count}/{steps} total={total.item():.6f} recon={recon.item():.6f} kl={kl.item():.6f} beta={beta:.3g} loss={lt} mask={mp:.3g}"
             )
 
     state["total_steps"] = int(state.get("total_steps", 0)) + step_count
@@ -106,6 +144,7 @@
         seq_len=seq_len,
         vocab_size=vocab_size,
         hidden_dim=hidden_dim,
+        loss_type=lt,
     )
 
     return last_total
@@ -128,6 +167,7 @@
     train_cfg: TrainingConfig,
     tokenizer: str,
     window_size_bp: int,
+    loss_type: Optional[str] = None,
 ) -> np.ndarray:
     if torch is None:
         raise RuntimeError("PyTorch not installed.")
@@ -136,6 +176,8 @@
     seq_len, vocab_size = tokenizer_meta(tokenizer, window_size_bp)
     hidden_dim = train_cfg.hidden_dim
 
+    lt = _default_loss_type(tokenizer) if loss_type is None else str(loss_type).lower()
+
     model, optimizer, global_step, ckpt_path = load_or_init_model(
         io_cfg=io_cfg,
         seq_len=seq_len,
@@ -144,6 +186,7 @@
         learning_rate=train_cfg.learning_rate,
         device=device,
         tokenizer=tokenizer,
+        loss_type=lt,
     )
 
     model.eval()
@@ -154,7 +197,18 @@
         N = wt.size(0)
         x_flat = wt.view(N, -1)
         mu, logvar = model.encode(x_flat)
-        recon_flat = model.decode(mu)
-        recon = recon_flat.view_as(wt)
-        mse = (recon - wt).pow(2).mean(dim=(1, 2))
-        return mse.cpu().numpy().astype(np.float32)
+        logits = model.decode(mu)
+        if lt == "ce":
+            # Negative log-likelihood per window (mean CE over positions)
+            if torch is None:
+                raise RuntimeError("PyTorch not installed.")
+            import torch.nn.functional as F
+            logits3 = logits.view(N, int(seq_len), int(vocab_size))
+            targets = wt.argmax(dim=2)
+            ce = F.cross_entropy(logits3.view(-1, int(vocab_size)), targets.view(-1), reduction="none")
+            ce_w = ce.view(N, int(seq_len)).mean(dim=1)
+            return ce_w.cpu().numpy().astype(np.float32)
+        else:
+            recon = torch.sigmoid(logits).view_as(wt)
+            mse = (recon - wt).pow(2).mean(dim=(1, 2))
+            return mse.cpu().numpy().astype(np.float32)
