diff -ruN ./VALIDATE_PROTEOME_MODE.md ../genostream_gb/VALIDATE_PROTEOME_MODE.md
--- ./VALIDATE_PROTEOME_MODE.md	2025-12-13 04:10:16.000000000 +0000
+++ VALIDATE_PROTEOME_MODE.md	2025-12-13 05:48:20.311022905 +0000
@@ -1,12 +1,16 @@
-# Genostream — Proteome (AA) mode quick validation
+# Genostream — Proteome (AA) mode quick validation (GenBank/CDS)
 
-This repo supports `--tokenizer aa`, which:
+This repo supports `--tokenizer aa` to train the VAE on **protein windows**.
 
-1. Fetches a plasmid FASTA
-2. Finds ORFs in all 6 frames (simple ATG→STOP scan)
-3. Translates ORFs to proteins (AA strings)
-4. Windows proteins into fixed AA windows
-5. Trains the same VAE on AA one-hot windows
+In AA mode, Genostream can derive proteins two ways:
+
+- **GenBank (recommended):** fetches a GenBank flatfile and extracts **CDS proteins** (prefers `/translation`; falls back to translating CDS coordinates from ORIGIN).
+- **FASTA fallback:** fetches FASTA and runs a naive 6-frame ORF finder.
+
+You can select the record source with `--source`:
+
+- Default: **FASTA** for `base` / `codon`
+- Default: **GenBank** for `aa`
 
 ## 0) One-time notes
 
@@ -20,17 +24,19 @@
 
 ## 1) Validate on 1 accession
 
-Pick a plasmid accession you know works (example uses `NC_002134.1` — replace if needed).
+Example uses `NC_002134.1` (replace if needed).
 
 ```bash
 cd ~/genostream
 source venv/bin/activate
 
-python3 stream_train.py fetch-one NC_002134.1
+# Optional: prefetch GenBank (AA mode will auto-fetch if missing)
+python3 stream_train.py fetch-one NC_002134.1 --source genbank
 
-# Train AA-VAE on ORF-derived protein windows
+# Train AA-VAE on CDS-derived protein windows
 python3 stream_train.py train-one NC_002134.1 \
   --tokenizer aa \
+  --source genbank \
   --min-orf-aa 90 \
   --window-size 256 \
   --stride 128 \
@@ -40,11 +46,19 @@
 # Visualize reconstruction error + side metric (hydrophobic fraction in AA mode)
 python3 stream_train.py scope-one NC_002134.1 \
   --tokenizer aa \
+  --source genbank \
   --min-orf-aa 90 \
   --window-size 256 \
   --stride 128
 ```
 
+If you want to compare CDS-vs-ORF quickly:
+
+```bash
+# ORF-based proteins from FASTA
+python3 stream_train.py train-one NC_002134.1 --tokenizer aa --source fasta --min-orf-aa 90 --window-size 256 --stride 128 --steps 50 --batch-size 16
+```
+
 ## 2) Minimal stream pass (10 plasmids)
 
 ```bash
@@ -53,6 +67,7 @@
 python3 stream_train.py stream \
   --catalog config/plasmids_10.txt \
   --tokenizer aa \
+  --source genbank \
   --min-orf-aa 90 \
   --window-size 256 \
   --stride 128 \
diff -ruN ./config/stream_config.yaml ../genostream_gb/config/stream_config.yaml
--- ./config/stream_config.yaml	2025-12-12 03:02:15.000000000 +0000
+++ config/stream_config.yaml	2025-12-13 05:47:20.789632725 +0000
@@ -16,6 +16,7 @@
 
 io:
   cache_fasta_dir: "cache/fasta"
+  cache_genbank_dir: "cache/genbank"
   cache_encoded_dir: "cache/encoded"
   model_dir: "model"
   checkpoints_dir: "model/checkpoints"
Binary files ./genostream/__pycache__/io_utils.cpython-311.pyc and ../genostream_gb/genostream/__pycache__/io_utils.cpython-311.pyc differ
diff -ruN ./genostream/cli.py ../genostream_gb/genostream/cli.py
--- ./genostream/cli.py	2025-12-13 04:00:38.000000000 +0000
+++ genostream/cli.py	2025-12-13 05:45:28.911219543 +0000
@@ -8,7 +8,7 @@
 from .encoding import compute_gc_from_encoded, encode_accession
 from .generate import generate_plasmid_sequence, generate_protein_sequence
 from .io_utils import ensure_dirs, load_state, read_catalog, save_state, setup_logging, encoded_cache_path
-from .ncbi_fetch import fetch_fasta
+from .ncbi_fetch import fetch_fasta, fetch_genbank
 from .training import cleanup_accession_files, compute_window_errors, train_on_encoded
 
 try:
@@ -28,6 +28,30 @@
     v = getattr(args, "min_orf_aa", None)
     return int(v if v is not None else train_cfg.min_orf_aa)
 
+
+def _get_source(args, tok: str) -> str:
+    v = getattr(args, "source", None)
+    if v is not None:
+        return str(v).lower()
+    # Default behavior:
+    #   - base/codon: FASTA (original pipeline)
+    #   - aa: GenBank (prefer CDS translations when available)
+    return "genbank" if tok == "aa" else "fasta"
+
+def _ensure_record(accession: str, src: str, io_cfg, ncbi_cfg, force: bool = False) -> str:
+    """Ensure the requested record exists in cache; fetch if missing."""
+    src = (src or "fasta").lower()
+    if src == "genbank":
+        gb_dir = getattr(io_cfg, "cache_genbank_dir", "cache/genbank")
+        gb_path = os.path.join(gb_dir, f"{accession}.gb")
+        if not os.path.exists(gb_path) or force:
+            fetch_genbank(accession, io_cfg, ncbi_cfg, force=force)
+        return gb_path
+    fasta_path = os.path.join(io_cfg.cache_fasta_dir, f"{accession}.fasta")
+    if not os.path.exists(fasta_path) or force:
+        fetch_fasta(accession, io_cfg, ncbi_cfg, force=force)
+    return fasta_path
+
 def _pick_window_stride(args, train_cfg, tok: str):
     if tok == "aa":
         w = args.window_size if args.window_size is not None else train_cfg.protein_window_aa
@@ -71,16 +95,23 @@
         print(f"    ... (+{len(accessions)-10} more)")
     return 0
 
+
 def cmd_fetch_one(args: argparse.Namespace) -> int:
     cfg = load_full_config(args.config)
     ncbi_cfg, train_cfg, io_cfg = extract_configs(cfg)
     ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)
-    fetch_fasta(args.accession, io_cfg, ncbi_cfg, force=args.force)
+
+    src = str(getattr(args, "source", None) or "fasta").lower()
+    if src == "genbank":
+        fetch_genbank(args.accession, io_cfg, ncbi_cfg, force=args.force)
+    else:
+        fetch_fasta(args.accession, io_cfg, ncbi_cfg, force=args.force)
     return 0
 
+
 def cmd_encode_one(args: argparse.Namespace) -> int:
     cfg = load_full_config(args.config)
-    _, train_cfg, io_cfg = extract_configs(cfg)
+    ncbi_cfg, train_cfg, io_cfg = extract_configs(cfg)
     ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)
 
     tok = _get_tok(args, train_cfg)
@@ -89,20 +120,24 @@
     window_size, stride = _pick_window_stride(args, train_cfg, tok)
     _validate_tok_params(tok, window_size, stride, frame)
 
-    fasta_path = os.path.join(io_cfg.cache_fasta_dir, f"{args.accession}.fasta")
-    if not os.path.exists(fasta_path):
-        print(f"FASTA for {args.accession} not found at {fasta_path}. Run fetch-one first.")
-        return 1
+    src = _get_source(args, tok)
+    _ensure_record(args.accession, src, io_cfg=io_cfg, ncbi_cfg=ncbi_cfg, force=False)
 
-    out_path = encoded_cache_path(io_cfg, args.accession, tok, window_size, stride, frame, min_orf_aa=(min_orf if tok=="aa" else None))
+    out_path = encoded_cache_path(
+        io_cfg, args.accession, tok, window_size, stride, frame,
+        source=src,
+        min_orf_aa=(min_orf if tok=="aa" else None),
+    )
     encoded = encode_accession(
         args.accession, io_cfg, window_size, stride,
         tokenizer=tok, frame_offset=frame, min_orf_aa=min_orf,
+        source=src,
         save_to_disk=True, out_path=out_path
     )
-    print(f"{args.accession}: encoded tokenizer={tok} -> shape={encoded.shape} saved={out_path}")
+    print(f"{args.accession}: encoded tokenizer={tok} source={src} -> shape={encoded.shape} saved={out_path}")
     return 0
 
+
 def cmd_train_one(args: argparse.Namespace) -> int:
     cfg = load_full_config(args.config)
     ncbi_cfg, train_cfg, io_cfg = extract_configs(cfg)
@@ -115,15 +150,18 @@
     window_size, stride = _pick_window_stride(args, train_cfg, tok)
     _validate_tok_params(tok, window_size, stride, frame)
 
+    src = _get_source(args, tok)
+
     batch_size = args.batch_size or train_cfg.batch_size
     steps = args.steps or train_cfg.steps_per_plasmid
 
-    fasta_path = os.path.join(io_cfg.cache_fasta_dir, f"{args.accession}.fasta")
-    if not os.path.exists(fasta_path):
-        logging.info(f"{args.accession}: FASTA not found; fetching.")
-        fetch_fasta(args.accession, io_cfg, ncbi_cfg, force=False)
+    _ensure_record(args.accession, src, io_cfg=io_cfg, ncbi_cfg=ncbi_cfg, force=False)
 
-    enc_path = encoded_cache_path(io_cfg, args.accession, tok, window_size, stride, frame, min_orf_aa=(min_orf if tok=="aa" else None))
+    enc_path = encoded_cache_path(
+        io_cfg, args.accession, tok, window_size, stride, frame,
+        source=src,
+        min_orf_aa=(min_orf if tok=="aa" else None),
+    )
     if os.path.exists(enc_path) and not args.reencode:
         encoded = np.load(enc_path)
         logging.info(f"{args.accession}: using cached encoded at {enc_path} shape={encoded.shape}")
@@ -131,6 +169,7 @@
         encoded = encode_accession(
             args.accession, io_cfg, window_size, stride,
             tokenizer=tok, frame_offset=frame, min_orf_aa=min_orf,
+            source=src,
             save_to_disk=True, out_path=enc_path
         )
 
@@ -138,21 +177,22 @@
         args.accession, encoded,
         steps=steps, batch_size=batch_size,
         state=state, io_cfg=io_cfg, train_cfg=train_cfg,
-        tokenizer=tok, window_size_bp=window_size,  # "units" for aa, ok
+        tokenizer=tok, window_size_bp=window_size,
     )
 
     pvc = state["plasmid_visit_counts"]
     pvc[args.accession] = pvc.get(args.accession, 0) + 1
     save_state(io_cfg.state_file, state)
 
-    print(f"{args.accession}: train-one tokenizer={tok} steps={steps} batch={batch_size} last_total={last_total:.6f}")
+    print(f"{args.accession}: train-one tokenizer={tok} source={src} steps={steps} batch={batch_size} last_total={last_total:.6f}")
     return 0
 
+
 def cmd_scope_one(args: argparse.Namespace) -> int:
     if curses is None:
         raise RuntimeError("curses not available")
     cfg = load_full_config(args.config)
-    _, train_cfg, io_cfg = extract_configs(cfg)
+    ncbi_cfg, train_cfg, io_cfg = extract_configs(cfg)
     ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)
 
     tok = _get_tok(args, train_cfg)
@@ -161,18 +201,26 @@
     window_size, stride = _pick_window_stride(args, train_cfg, tok)
     _validate_tok_params(tok, window_size, stride, frame)
 
-    enc_path = encoded_cache_path(io_cfg, args.accession, tok, window_size, stride, frame, min_orf_aa=(min_orf if tok=="aa" else None))
+    src = _get_source(args, tok)
+    _ensure_record(args.accession, src, io_cfg=io_cfg, ncbi_cfg=ncbi_cfg, force=False)
+
+    enc_path = encoded_cache_path(
+        io_cfg, args.accession, tok, window_size, stride, frame,
+        source=src,
+        min_orf_aa=(min_orf if tok=="aa" else None),
+    )
     if os.path.exists(enc_path) and not args.reencode:
         encoded = np.load(enc_path)
     else:
         encoded = encode_accession(
             args.accession, io_cfg, window_size, stride,
             tokenizer=tok, frame_offset=frame, min_orf_aa=min_orf,
+            source=src,
             save_to_disk=True, out_path=enc_path
         )
 
     errors = compute_window_errors(args.accession, encoded, io_cfg=io_cfg, train_cfg=train_cfg, tokenizer=tok, window_size_bp=window_size)
-    metric = compute_gc_from_encoded(encoded, tokenizer=tok)  # for aa: hydrophobic fraction
+    metric = compute_gc_from_encoded(encoded, tokenizer=tok)
 
     curses.wrapper(
         run_scope_ui,
@@ -185,11 +233,12 @@
     )
     return 0
 
+
 def cmd_scope_stream(args: argparse.Namespace) -> int:
     if curses is None:
         raise RuntimeError("curses not available")
     cfg = load_full_config(args.config)
-    _, train_cfg, io_cfg = extract_configs(cfg)
+    ncbi_cfg, train_cfg, io_cfg = extract_configs(cfg)
     ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)
 
     tok = _get_tok(args, train_cfg)
@@ -198,16 +247,24 @@
     window_size, stride = _pick_window_stride(args, train_cfg, tok)
     _validate_tok_params(tok, window_size, stride, frame)
 
+    src = _get_source(args, tok)
+    _ensure_record(args.accession, src, io_cfg=io_cfg, ncbi_cfg=ncbi_cfg, force=False)
+
     steps = args.steps or train_cfg.steps_per_plasmid
     batch_size = args.batch_size or train_cfg.batch_size
 
-    enc_path = encoded_cache_path(io_cfg, args.accession, tok, window_size, stride, frame, min_orf_aa=(min_orf if tok=="aa" else None))
+    enc_path = encoded_cache_path(
+        io_cfg, args.accession, tok, window_size, stride, frame,
+        source=src,
+        min_orf_aa=(min_orf if tok=="aa" else None),
+    )
     if os.path.exists(enc_path) and not args.reencode:
         encoded = np.load(enc_path)
     else:
         encoded = encode_accession(
             args.accession, io_cfg, window_size, stride,
             tokenizer=tok, frame_offset=frame, min_orf_aa=min_orf,
+            source=src,
             save_to_disk=True, out_path=enc_path
         )
 
@@ -254,6 +311,7 @@
     )
     return 0
 
+
 def cmd_stream(args: argparse.Namespace) -> int:
     import random
     cfg = load_full_config(args.config)
@@ -269,6 +327,8 @@
     window_size, stride = _pick_window_stride(args, train_cfg, tok)
     _validate_tok_params(tok, window_size, stride, frame)
 
+    src = _get_source(args, tok)
+
     batch_size = args.batch_size or train_cfg.batch_size
     steps_per_plasmid = args.steps_per_plasmid or train_cfg.steps_per_plasmid
     max_epochs = args.max_epochs or train_cfg.max_stream_epochs
@@ -283,17 +343,20 @@
         for idx in indices:
             acc = accessions[idx]
 
-            fasta_path = os.path.join(io_cfg.cache_fasta_dir, f"{acc}.fasta")
-            if not os.path.exists(fasta_path):
-                fetch_fasta(acc, io_cfg, ncbi_cfg, force=False)
+            _ensure_record(acc, src, io_cfg=io_cfg, ncbi_cfg=ncbi_cfg, force=False)
 
-            enc_path = encoded_cache_path(io_cfg, acc, tok, window_size, stride, frame, min_orf_aa=(min_orf if tok=="aa" else None))
+            enc_path = encoded_cache_path(
+                io_cfg, acc, tok, window_size, stride, frame,
+                source=src,
+                min_orf_aa=(min_orf if tok=="aa" else None),
+            )
             if os.path.exists(enc_path):
                 encoded = np.load(enc_path)
             else:
                 encoded = encode_accession(
                     acc, io_cfg, window_size, stride,
                     tokenizer=tok, frame_offset=frame, min_orf_aa=min_orf,
+                    source=src,
                     save_to_disk=True, out_path=enc_path
                 )
 
@@ -382,12 +445,14 @@
 
     s = sub.add_parser("fetch-one")
     s.add_argument("accession"); s.add_argument("--force", action="store_true")
+    s.add_argument("--source", choices=["fasta","genbank"], default=None, help="Fetch record source (default: fasta)")
     s.set_defaults(func=cmd_fetch_one)
 
     def add_tok_args(sp):
         sp.add_argument("--tokenizer", choices=["base","codon","aa"], default=None, help="Override tokenizer (default from config)")
         sp.add_argument("--frame-offset", type=int, choices=[0,1,2], default=None, help="Codon frame offset (default from config)")
         sp.add_argument("--min-orf-aa", type=int, default=None, help="AA tokenizer: minimum ORF length in amino acids (default from config)")
+        sp.add_argument("--source", choices=["fasta","genbank"], default=None, help="Sequence record source. Default: fasta for base/codon, genbank for aa.")
 
     s = sub.add_parser("encode-one")
     s.add_argument("accession")
diff -ruN ./genostream/config.py ../genostream_gb/genostream/config.py
--- ./genostream/config.py	2025-12-13 04:00:38.000000000 +0000
+++ genostream/config.py	2025-12-13 05:42:03.587956171 +0000
@@ -34,6 +34,7 @@
     },
     "io": {
         "cache_fasta_dir": "cache/fasta",
+        "cache_genbank_dir": "cache/genbank",
         "cache_encoded_dir": "cache/encoded",
         "model_dir": "model",
         "checkpoints_dir": "model/checkpoints",
@@ -73,6 +74,7 @@
 @dataclass
 class IOConfig:
     cache_fasta_dir: str
+    cache_genbank_dir: str
     cache_encoded_dir: str
     model_dir: str
     checkpoints_dir: str
@@ -132,6 +134,7 @@
         ),
         IOConfig(
             cache_fasta_dir=io.get("cache_fasta_dir", "cache/fasta"),
+            cache_genbank_dir=io.get("cache_genbank_dir", "cache/genbank"),
             cache_encoded_dir=io.get("cache_encoded_dir", "cache/encoded"),
             model_dir=io.get("model_dir", "model"),
             checkpoints_dir=io.get("checkpoints_dir", "model/checkpoints"),
diff -ruN ./genostream/encoding.py ../genostream_gb/genostream/encoding.py
--- ./genostream/encoding.py	2025-12-13 04:00:38.000000000 +0000
+++ genostream/encoding.py	2025-12-13 05:43:07.113228546 +0000
@@ -124,6 +124,248 @@
     comp = {"A":"T","C":"G","G":"C","T":"A","N":"N"}
     return "".join(comp.get(b, "N") for b in reversed(seq.upper()))
 
+
+def parse_genbank_dna(path: str) -> str:
+    """Extract the DNA sequence from a GenBank flatfile (ORIGIN section)."""
+    if not os.path.exists(path):
+        raise FileNotFoundError(f"GenBank not found: {path}")
+    seq_parts: List[str] = []
+    in_origin = False
+    with open(path, "r", encoding="utf-8", errors="ignore") as f:
+        for line in f:
+            if line.startswith("ORIGIN"):
+                in_origin = True
+                continue
+            if in_origin:
+                if line.startswith("//"):
+                    break
+                # lines look like: '     1 atgc...'
+                s = "".join(ch for ch in line.strip() if ch.isalpha())
+                if s:
+                    seq_parts.append(s)
+    seq = "".join(seq_parts).upper().replace("U", "T")
+    if not seq:
+        raise ValueError(f"No ORIGIN sequence found in {path}")
+    return seq
+
+def _split_top_level_commas(s: str) -> List[str]:
+    parts: List[str] = []
+    depth = 0
+    cur: List[str] = []
+    for ch in s:
+        if ch == "(":
+            depth += 1
+        elif ch == ")":
+            depth = max(0, depth - 1)
+        if ch == "," and depth == 0:
+            parts.append("".join(cur).strip())
+            cur = []
+        else:
+            cur.append(ch)
+    if cur:
+        parts.append("".join(cur).strip())
+    return [p for p in parts if p]
+
+def _parse_loc_range(token: str) -> Tuple[int, int]:
+    # Handles: 123..456, <123..>456, 123, 123^124 (we treat as 123..124)
+    t = token.strip()
+    t = t.replace("<", "").replace(">", "")
+    if "^" in t:
+        a, b = t.split("^", 1)
+        a = a.strip(); b = b.strip()
+        return int(a), int(b)
+    if ".." in t:
+        a, b = t.split("..", 1)
+        return int(a.strip()), int(b.strip())
+    return int(t), int(t)
+
+def extract_dna_from_location(dna: str, loc: str) -> str:
+    """Extract CDS DNA from a GenBank location string using the ORIGIN DNA."""
+    s = loc.strip().replace(" ", "")
+    def parse_expr(expr: str) -> Tuple[str, bool]:
+        # returns (sequence, is_complement)
+        if expr.startswith("complement(") and expr.endswith(")"):
+            inner = expr[len("complement("):-1]
+            seq0, _ = parse_expr(inner)
+            return seq0, True
+        if (expr.startswith("join(") or expr.startswith("order(")) and expr.endswith(")"):
+            inner = expr[expr.find("(")+1:-1]
+            segs = _split_top_level_commas(inner)
+            out = []
+            for seg in segs:
+                seq1, comp1 = parse_expr(seg)
+                if comp1:
+                    # If any segment returns complement, apply at segment level
+                    seq1 = reverse_complement(seq1)
+                out.append(seq1)
+            return "".join(out), False
+        # simple range
+        start, end = _parse_loc_range(expr)
+        if start < 1 or end < 1 or end < start:
+            return "", False
+        # GenBank is 1-based inclusive
+        return dna[start-1:end], False
+
+    seq, is_comp = parse_expr(s)
+    if is_comp:
+        seq = reverse_complement(seq)
+    return seq
+
+def translate_cds(dna: str, codon_start: int = 1) -> str:
+    """Translate CDS DNA to AA; respects codon_start (1..3)."""
+    if codon_start not in (1, 2, 3):
+        codon_start = 1
+    dna = dna.upper().replace("U", "T")
+    if codon_start > 1 and len(dna) >= codon_start - 1:
+        dna = dna[codon_start - 1 :]
+    aas: List[str] = []
+    for i in range(0, len(dna) - 2, 3):
+        cod = dna[i:i+3]
+        if cod in STOP_CODONS:
+            break
+        aas.append(CODON_TO_AA.get(cod, "X"))
+    return "".join(aas)
+
+def extract_cds_proteins_from_genbank(path: str, min_aa: int = 90) -> List[str]:
+    """
+    Extract protein sequences from GenBank FEATURES/CDS.
+
+    Preference order per CDS:
+      1) /translation="..." qualifier (most reliable)
+      2) location + ORIGIN DNA + /codon_start (fallback)
+
+    Notes:
+      - Skips CDS with /pseudo or /pseudogene.
+      - Handles common location forms: 123..456, complement(123..456), join(...), complement(join(...)).
+    """
+    if not os.path.exists(path):
+        raise FileNotFoundError(f"GenBank not found: {path}")
+
+    # Parse ORIGIN DNA once for fallback translation
+    try:
+        origin_dna = parse_genbank_dna(path)
+    except Exception:
+        origin_dna = ""
+
+    proteins: List[str] = []
+    in_features = False
+    in_origin = False
+
+    cur_loc: Optional[str] = None
+    cur_translation: Optional[str] = None
+    cur_codon_start: int = 1
+    cur_is_pseudo: bool = False
+    reading_translation = False
+    trans_buf: List[str] = []
+
+    def flush_cds():
+        nonlocal cur_loc, cur_translation, cur_codon_start, cur_is_pseudo, reading_translation, trans_buf
+        if cur_loc is None:
+            return
+        if cur_is_pseudo:
+            # reset
+            cur_loc = None; cur_translation = None; cur_codon_start = 1; cur_is_pseudo = False
+            reading_translation = False; trans_buf = []
+            return
+
+        prot = ""
+        if cur_translation:
+            prot = cur_translation
+        elif origin_dna:
+            cds_dna = extract_dna_from_location(origin_dna, cur_loc)
+            if cds_dna:
+                prot = translate_cds(cds_dna, codon_start=cur_codon_start)
+
+        prot = prot.replace(" ", "").replace("\n", "").strip().upper()
+        # Remove trailing '*' if present
+        if prot.endswith("*"):
+            prot = prot[:-1]
+        if prot and len(prot) >= min_aa:
+            proteins.append(prot)
+
+        # reset
+        cur_loc = None; cur_translation = None; cur_codon_start = 1; cur_is_pseudo = False
+        reading_translation = False; trans_buf = []
+
+    with open(path, "r", encoding="utf-8", errors="ignore") as f:
+        for line in f:
+            if line.startswith("FEATURES"):
+                in_features = True
+                continue
+            if line.startswith("ORIGIN"):
+                in_origin = True
+                # end of FEATURES parsing
+                in_features = False
+                # flush any CDS we were building
+                if reading_translation and trans_buf:
+                    cur_translation = "".join(trans_buf)
+                flush_cds()
+                continue
+            if in_origin:
+                # no need to parse further here
+                continue
+            if not in_features:
+                continue
+
+            # New feature begins at column 6 (5 spaces) with a key
+            if line.startswith("     ") and len(line) > 5 and line[5] != " ":
+                # starting a new feature; flush previous CDS if any
+                if reading_translation and trans_buf:
+                    cur_translation = "".join(trans_buf)
+                flush_cds()
+
+                key = line[5:21].strip()
+                loc = line[21:].strip()
+                if key == "CDS":
+                    cur_loc = loc
+                    cur_translation = None
+                    cur_codon_start = 1
+                    cur_is_pseudo = False
+                    reading_translation = False
+                    trans_buf = []
+                else:
+                    cur_loc = None
+                continue
+
+            # Qualifiers: start around column 22 (21 spaces)
+            if cur_loc is not None and line.startswith("                     "):
+                q = line.strip()
+
+                if reading_translation:
+                    # keep accumulating until closing quote
+                    if q.endswith('"'):
+                        trans_buf.append(q.rstrip('"'))
+                        cur_translation = "".join(trans_buf)
+                        reading_translation = False
+                        trans_buf = []
+                    else:
+                        trans_buf.append(q)
+                    continue
+
+                if q.startswith("/pseudo") or q.startswith("/pseudogene"):
+                    cur_is_pseudo = True
+                    continue
+                if q.startswith("/codon_start="):
+                    try:
+                        cur_codon_start = int(q.split("=", 1)[1].strip().strip('"'))
+                    except Exception:
+                        cur_codon_start = 1
+                    continue
+                if q.startswith("/translation="):
+                    val = q.split("=", 1)[1].lstrip()
+                    # translation is quoted and can span lines
+                    if val.startswith('"'):
+                        val = val[1:]
+                    if val.endswith('"'):
+                        cur_translation = val.rstrip('"')
+                        reading_translation = False
+                        trans_buf = []
+                    else:
+                        reading_translation = True
+                        trans_buf = [val]
+                    continue
+
+    return proteins
 # -----------------------------
 # Base encoding
 # -----------------------------
@@ -318,15 +560,24 @@
     tokenizer: str = "base",
     frame_offset: int = 0,
     min_orf_aa: int = 90,
+    source: str = "fasta",
     save_to_disk: bool = True,
     out_path: Optional[str] = None,
 ) -> np.ndarray:
-    fasta_path = os.path.join(io_cfg.cache_fasta_dir, f"{accession}.fasta")
-    if not os.path.exists(fasta_path):
-        raise FileNotFoundError(f"FASTA for {accession} not found at {fasta_path}")
-
-    seq = parse_fasta_sequence(fasta_path)
     tok = tokenizer.lower()
+    src = source.lower()
+    if src == "genbank":
+        gb_path = os.path.join(getattr(io_cfg, "cache_genbank_dir", "cache/genbank"), f"{accession}.gb")
+        if not os.path.exists(gb_path):
+            raise FileNotFoundError(f"GenBank for {accession} not found at {gb_path}")
+        seq = parse_genbank_dna(gb_path)
+        gb_for_proteins = gb_path
+    else:
+        fasta_path = os.path.join(io_cfg.cache_fasta_dir, f"{accession}.fasta")
+        if not os.path.exists(fasta_path):
+            raise FileNotFoundError(f"FASTA for {accession} not found at {fasta_path}")
+        seq = parse_fasta_sequence(fasta_path)
+        gb_for_proteins = None
 
     if tok == "base":
         encoded = encode_sequence_one_hot(seq, window_size, stride)
@@ -337,10 +588,22 @@
         logging.info(f"{accession}: encoded(CODON) len={len(seq)} -> windows={encoded.shape[0]} shape={encoded.shape}")
 
     elif tok == "aa":
-        proteins = find_orfs_proteins(seq, min_orf_aa=min_orf_aa)
+        proteins: List[str] = []
+        if src == "genbank" and gb_for_proteins is not None:
+            try:
+                proteins = extract_cds_proteins_from_genbank(gb_for_proteins, min_aa=min_orf_aa)
+                if proteins:
+                    logging.info(f"{accession}: extracted CDS proteins from GenBank: {len(proteins)} (min_aa={min_orf_aa})")
+                else:
+                    logging.warning(f"{accession}: GenBank contained no CDS translations >= {min_orf_aa}aa; falling back to naive ORFs.")
+            except Exception as e:
+                logging.warning(f"{accession}: failed to parse GenBank CDS ({e}); falling back to naive ORFs.")
+                proteins = []
+        if not proteins:
+            proteins = find_orfs_proteins(seq, min_orf_aa=min_orf_aa)
         encoded = encode_proteins_aa_windows(proteins, window_aa=window_size, stride_aa=stride)
         logging.info(
-            f"{accession}: encoded(AA) genome_len={len(seq)} proteins={len(proteins)} "
+            f"{accession}: encoded(AA) src={src} genome_len={len(seq)} proteins={len(proteins)} "
             f"-> windows={encoded.shape[0]} window_aa={window_size} stride_aa={stride} shape={encoded.shape}"
         )
 
diff -ruN ./genostream/io_utils.py ../genostream_gb/genostream/io_utils.py
--- ./genostream/io_utils.py	2025-12-13 04:00:38.000000000 +0000
+++ genostream/io_utils.py	2025-12-13 05:49:52.975570760 +0000
@@ -25,6 +25,7 @@
 
 def ensure_dirs(io_cfg: IOConfig) -> None:
     os.makedirs(io_cfg.cache_fasta_dir, exist_ok=True)
+    os.makedirs(getattr(io_cfg, 'cache_genbank_dir', 'cache/genbank'), exist_ok=True)
     os.makedirs(io_cfg.cache_encoded_dir, exist_ok=True)
     os.makedirs(io_cfg.model_dir, exist_ok=True)
     os.makedirs(io_cfg.checkpoints_dir, exist_ok=True)
@@ -78,6 +79,7 @@
     window_size: int,
     stride: int,
     frame_offset: int,
+    source: str = "fasta",
     min_orf_aa: int | None = None,
 ) -> str:
     """
@@ -90,6 +92,13 @@
     """
     tok = tokenizer.lower()
     tag = f"{tok}.w{int(window_size)}.s{int(stride)}"
+    src = source.lower()
+    if src == "fasta":
+        tag += ".srcfa"
+    elif src == "genbank":
+        tag += ".srcgb"
+    else:
+        tag += f".src{src}"
     if tok == "codon":
         tag += f".f{int(frame_offset)}"
     if tok == "aa" and min_orf_aa is not None:
diff -ruN ./genostream/ncbi_fetch.py ../genostream_gb/genostream/ncbi_fetch.py
--- ./genostream/ncbi_fetch.py	2025-12-12 10:22:58.000000000 +0000
+++ genostream/ncbi_fetch.py	2025-12-13 05:42:21.164154735 +0000
@@ -63,3 +63,64 @@
             time.sleep(delay)
 
     raise RuntimeError(f"Failed to fetch {accession} from NCBI: {last_error}")
+
+
+def fetch_genbank(
+    accession: str,
+    io_cfg: IOConfig,
+    ncbi_cfg: NCBIConfig,
+    force: bool = False,
+    rettype: str = "gbwithparts",
+) -> str:
+    """Download GenBank flatfile for accession from NCBI nuccore, returning local path."""
+    out_path = os.path.join(getattr(io_cfg, "cache_genbank_dir", "cache/genbank"), f"{accession}.gb")
+    fetch_logger = logging.getLogger("fetch")
+
+    if os.path.exists(out_path) and not force:
+        fetch_logger.info(f"{accession}: using cached GenBank at {out_path}")
+        return out_path
+
+    url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
+    params = {
+        "db": "nuccore",
+        "id": accession,
+        "rettype": rettype,
+        "retmode": "text",
+        "email": ncbi_cfg.email,
+    }
+    if ncbi_cfg.api_key:
+        params["api_key"] = ncbi_cfg.api_key  # type: ignore[assignment]
+
+    fetch_logger.info(f"{accession}: fetching GenBank from NCBI {url}")
+    last_error: Optional[str] = None
+    for attempt in range(ncbi_cfg.max_retries + 1):
+        try:
+            resp = requests.get(url, params=params, timeout=30)
+            # GenBank flatfile usually begins with 'LOCUS'
+            if resp.status_code == 200 and resp.text.lstrip().startswith("LOCUS"):
+                os.makedirs(os.path.dirname(out_path), exist_ok=True)
+                with open(out_path, "w", encoding="utf-8") as f:
+                    f.write(resp.text)
+                fetch_logger.info(
+                    f"{accession}: fetched and saved GenBank ({len(resp.text)} bytes)"
+                )
+                return out_path
+            else:
+                msg = (
+                    f"HTTP {resp.status_code}, "
+                    f"first 80 chars: {resp.text[:80]!r}"
+                )
+                last_error = msg
+        except Exception as e:  # noqa: BLE001
+            last_error = str(e)
+
+        if attempt < ncbi_cfg.max_retries:
+            delay = ncbi_cfg.backoff_seconds * (2 ** attempt)
+            fetch_logger.warning(
+                f"{accession}: GenBank fetch attempt {attempt+1} failed ({last_error}); "
+                f"retrying in {delay:.1f}s"
+            )
+            import time
+            time.sleep(delay)
+
+    raise RuntimeError(f"Failed to fetch GenBank for {accession} from NCBI: {last_error}")
diff -ruN ./genostream/training.py ../genostream_gb/genostream/training.py
--- ./genostream/training.py	2025-12-13 03:51:11.000000000 +0000
+++ genostream/training.py	2025-12-13 05:43:25.462693678 +0000
@@ -112,7 +112,8 @@
 
 def cleanup_accession_files(accession: str, io_cfg: IOConfig, encoded_path: str) -> None:
     fasta_path = os.path.join(io_cfg.cache_fasta_dir, f"{accession}.fasta")
-    for path in (fasta_path, encoded_path):
+    gb_path = os.path.join(getattr(io_cfg, 'cache_genbank_dir', 'cache/genbank'), f"{accession}.gb")
+    for path in (fasta_path, gb_path, encoded_path):
         if os.path.exists(path):
             try:
                 os.remove(path)
diff -ruN ./stream_config.yaml ../genostream_gb/stream_config.yaml
--- ./stream_config.yaml	2025-12-12 10:27:25.000000000 +0000
+++ stream_config.yaml	2025-12-13 05:47:20.788904788 +0000
@@ -19,6 +19,7 @@
 
 io:
   cache_fasta_dir: "cache/fasta"
+  cache_genbank_dir: "cache/genbank"
   cache_encoded_dir: "cache/encoded"
   model_dir: "model"
   checkpoints_dir: "model/checkpoints"
