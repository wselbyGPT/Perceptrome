#!/usr/bin/env python3
import argparse, logging, os
from typing import Any

import numpy as np

from .config import extract_configs, load_full_config
from .encoding import compute_gc_from_encoded, encode_accession
from .generate import generate_plasmid_sequence, generate_protein_sequence
from .io_utils import ensure_dirs, load_state, read_catalog, save_state, setup_logging, encoded_cache_path
from .ncbi_fetch import fetch_fasta, fetch_genbank
from .training import cleanup_accession_files, compute_window_errors, train_on_encoded

try:
    import curses
except ImportError:
    curses = None  # type: ignore

from .scope import run_scope_ui, run_scope_stream_ui, ScopeStreamContext

def _get_tok(args, train_cfg):
    return (getattr(args, "tokenizer", None) or train_cfg.tokenizer).lower()

def _get_frame(args, train_cfg):
    return int(getattr(args, "frame_offset", None) if getattr(args, "frame_offset", None) is not None else train_cfg.frame_offset)

def _get_min_orf(args, train_cfg):
    v = getattr(args, "min_orf_aa", None)
    return int(v if v is not None else train_cfg.min_orf_aa)


def _get_source(args, tok: str) -> str:
    v = getattr(args, "source", None)
    if v is not None:
        return str(v).lower()
    # Default behavior:
    #   - base/codon: FASTA (original pipeline)
    #   - aa: GenBank (prefer CDS translations when available)
    return "genbank" if tok == "aa" else "fasta"

def _ensure_record(accession: str, src: str, io_cfg, ncbi_cfg, force: bool = False) -> str:
    """Ensure the requested record exists in cache; fetch if missing."""
    src = (src or "fasta").lower()
    if src == "genbank":
        gb_dir = getattr(io_cfg, "cache_genbank_dir", "cache/genbank")
        gb_path = os.path.join(gb_dir, f"{accession}.gb")
        if not os.path.exists(gb_path) or force:
            fetch_genbank(accession, io_cfg, ncbi_cfg, force=force)
        return gb_path
    fasta_path = os.path.join(io_cfg.cache_fasta_dir, f"{accession}.fasta")
    if not os.path.exists(fasta_path) or force:
        fetch_fasta(accession, io_cfg, ncbi_cfg, force=force)
    return fasta_path

def _pick_window_stride(args, train_cfg, tok: str):
    if tok == "aa":
        w = args.window_size if args.window_size is not None else train_cfg.protein_window_aa
        s = args.stride if args.stride is not None else train_cfg.protein_stride_aa
    else:
        w = args.window_size if args.window_size is not None else train_cfg.window_size
        s = args.stride if args.stride is not None else train_cfg.stride
    return int(w), int(s)

def _validate_tok_params(tokenizer: str, window_size: int, stride: int, frame: int):
    if tokenizer == "codon":
        if window_size % 3 != 0:
            raise ValueError(f"--tokenizer codon requires window_size divisible by 3 (got {window_size})")
        if stride % 3 != 0:
            raise ValueError(f"--tokenizer codon requires stride divisible by 3 (got {stride})")
        if frame not in (0,1,2):
            raise ValueError("--frame-offset must be 0,1,2")
    elif tokenizer == "aa":
        if window_size <= 0 or stride <= 0:
            raise ValueError("--tokenizer aa requires positive --window-size/--stride (amino acids)")
    else:
        if window_size <= 0 or stride <= 0:
            raise ValueError("window_size/stride must be positive")



def _resolve_proteome_params(args: argparse.Namespace, train_cfg, state, tok: str, src: str) -> dict[str, Any]:
    """Resolve proteome-related knobs (curriculum, balanced sampling, denoising).

    Applied only for tok=="aa" and src=="genbank". CLI flags override config, which overrides curriculum.
    """
    tok = (tok or "").lower()
    src = (src or "").lower()

    # Defaults from config
    pol: dict[str, Any] = {
        "protein_len_min": getattr(train_cfg, "protein_len_min", None),
        "protein_len_max": getattr(train_cfg, "protein_len_max", None),
        "translation_only": bool(getattr(train_cfg, "translation_only", False)),
        "max_windows_per_protein": getattr(train_cfg, "max_windows_per_protein", None),
        "mask_prob": float(getattr(train_cfg, "aa_mask_prob", 0.05)) if tok == "aa" else 0.0,
        "span_mask_prob": float(getattr(train_cfg, "aa_span_mask_prob", 0.0)),
        "span_mask_len": int(getattr(train_cfg, "aa_span_mask_len", 0)),
        "curriculum_tag": None,
    }

    total_steps = int(state.get("total_steps", 0)) if isinstance(state, dict) else 0

    # Curriculum (optional)
    if (
        tok == "aa"
        and src == "genbank"
        and not bool(getattr(args, "no_curriculum", False))
        and bool(getattr(train_cfg, "curriculum_enabled", False))
    ):
        phases = list(getattr(train_cfg, "curriculum_phases", []) or [])
        steps = list(getattr(train_cfg, "curriculum_steps", []) or [])
        if phases:
            # Determine phase index by total_steps
            idx = 0
            if steps:
                for j, s in enumerate(steps):
                    try:
                        if total_steps >= int(s):
                            idx = j
                    except Exception:
                        pass
            idx = max(0, min(idx, len(phases) - 1))
            phase = phases[idx] if idx < len(phases) else {}
            if isinstance(phase, dict):
                for k in ("protein_len_min", "protein_len_max", "translation_only", "max_windows_per_protein", "mask_prob", "span_mask_prob", "span_mask_len"):
                    if k in phase and phase[k] is not None:
                        pol[k] = phase[k]
            pol["curriculum_tag"] = f"cur{idx}"

    # CLI overrides (only override if user explicitly set the flag)
    if getattr(args, "protein_len_min", None) is not None:
        pol["protein_len_min"] = int(getattr(args, "protein_len_min"))
    if getattr(args, "protein_len_max", None) is not None:
        pol["protein_len_max"] = int(getattr(args, "protein_len_max"))
    if getattr(args, "max_windows_per_protein", None) is not None:
        pol["max_windows_per_protein"] = int(getattr(args, "max_windows_per_protein"))

    # translation-only tri-state: --translation-only / --allow-translated
    if getattr(args, "translation_only", None) is True:
        pol["translation_only"] = True
    if getattr(args, "allow_translated", None) is True:
        pol["translation_only"] = False

    if getattr(args, "mask_prob", None) is not None:
        pol["mask_prob"] = float(getattr(args, "mask_prob"))
    if getattr(args, "span_mask_prob", None) is not None:
        pol["span_mask_prob"] = float(getattr(args, "span_mask_prob"))
    if getattr(args, "span_mask_len", None) is not None:
        pol["span_mask_len"] = int(getattr(args, "span_mask_len"))

    return pol

def cmd_init(args: argparse.Namespace) -> int:
    cfg = load_full_config(args.config)
    _, _, io_cfg = extract_configs(cfg)
    ensure_dirs(io_cfg)
    setup_logging(io_cfg.logs_dir)
    state = {"current_index": 0, "total_steps": 0, "plasmid_visit_counts": {}, "epoch": 0, "last_checkpoint": None}
    save_state(io_cfg.state_file, state)
    print(f"Initialized project. State file at: {io_cfg.state_file}")
    return 0

def cmd_catalog_show(args: argparse.Namespace) -> int:
    accessions = read_catalog(args.path)
    print(f"Catalog: {args.path}\n  {len(accessions)} accessions")
    for acc in accessions[:10]:
        print(f"    {acc}")
    if len(accessions) > 10:
        print(f"    ... (+{len(accessions)-10} more)")
    return 0


def cmd_fetch_one(args: argparse.Namespace) -> int:
    cfg = load_full_config(args.config)
    ncbi_cfg, train_cfg, io_cfg = extract_configs(cfg)
    ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)

    src = str(getattr(args, "source", None) or "fasta").lower()
    if src == "genbank":
        fetch_genbank(args.accession, io_cfg, ncbi_cfg, force=args.force)
    else:
        fetch_fasta(args.accession, io_cfg, ncbi_cfg, force=args.force)
    return 0


def cmd_encode_one(args: argparse.Namespace) -> int:
    cfg = load_full_config(args.config)
    ncbi_cfg, train_cfg, io_cfg = extract_configs(cfg)
    ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)

    tok = _get_tok(args, train_cfg)
    frame = _get_frame(args, train_cfg)
    min_orf = _get_min_orf(args, train_cfg)
    window_size, stride = _pick_window_stride(args, train_cfg, tok)
    _validate_tok_params(tok, window_size, stride, frame)

    src = _get_source(args, tok)
    pol = _resolve_proteome_params(args, train_cfg, state=None, tok=tok, src=src)
    _ensure_record(args.accession, src, io_cfg=io_cfg, ncbi_cfg=ncbi_cfg, force=False)

    out_path = encoded_cache_path(
        io_cfg, args.accession, tok, window_size, stride, frame,
        source=src,
        min_orf_aa=(min_orf if tok=="aa" else None),
        max_windows_per_protein=(pol.get("max_windows_per_protein") if tok=="aa" else None),
        protein_len_min=(pol.get("protein_len_min") if tok=="aa" else None),
        protein_len_max=(pol.get("protein_len_max") if tok=="aa" else None),
        translation_only=bool(pol.get("translation_only", False)) if tok=="aa" else False,
        curriculum_tag=pol.get("curriculum_tag"),
    )
    encoded = encode_accession(
        args.accession, io_cfg, window_size, stride,
        tokenizer=tok, frame_offset=frame, min_orf_aa=min_orf,
        source=src,
        max_windows_per_protein=(pol.get("max_windows_per_protein") if tok=="aa" else None),
        protein_len_min=(pol.get("protein_len_min") if tok=="aa" else None),
        protein_len_max=(pol.get("protein_len_max") if tok=="aa" else None),
        translation_only=bool(pol.get("translation_only", False)) if tok=="aa" else False,
        save_to_disk=True, out_path=out_path
    )
    print(f"{args.accession}: encoded tokenizer={tok} source={src} -> shape={encoded.shape} saved={out_path}")
    return 0


def cmd_train_one(args: argparse.Namespace) -> int:
    cfg = load_full_config(args.config)
    ncbi_cfg, train_cfg, io_cfg = extract_configs(cfg)
    ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)
    state = load_state(io_cfg.state_file)

    tok = _get_tok(args, train_cfg)
    frame = _get_frame(args, train_cfg)
    min_orf = _get_min_orf(args, train_cfg)
    window_size, stride = _pick_window_stride(args, train_cfg, tok)
    _validate_tok_params(tok, window_size, stride, frame)

    src = _get_source(args, tok)

    pol = _resolve_proteome_params(args, train_cfg, state=state, tok=tok, src=src)

    batch_size = args.batch_size or train_cfg.batch_size
    steps = args.steps or train_cfg.steps_per_plasmid

    _ensure_record(args.accession, src, io_cfg=io_cfg, ncbi_cfg=ncbi_cfg, force=False)

    enc_path = encoded_cache_path(
        io_cfg, args.accession, tok, window_size, stride, frame,
        source=src,
        min_orf_aa=(min_orf if tok=="aa" else None),
        max_windows_per_protein=(pol.get("max_windows_per_protein") if tok=="aa" else None),
        protein_len_min=(pol.get("protein_len_min") if tok=="aa" else None),
        protein_len_max=(pol.get("protein_len_max") if tok=="aa" else None),
        translation_only=bool(pol.get("translation_only", False)) if tok=="aa" else False,
        curriculum_tag=pol.get("curriculum_tag"),
    )
    if os.path.exists(enc_path) and not args.reencode:
        encoded = np.load(enc_path)
        logging.info(f"{args.accession}: using cached encoded at {enc_path} shape={encoded.shape}")
    else:
        encoded = encode_accession(
            args.accession, io_cfg, window_size, stride,
            tokenizer=tok, frame_offset=frame, min_orf_aa=min_orf,
            source=src,
            max_windows_per_protein=(pol.get("max_windows_per_protein") if tok=="aa" else None),
            protein_len_min=(pol.get("protein_len_min") if tok=="aa" else None),
            protein_len_max=(pol.get("protein_len_max") if tok=="aa" else None),
            translation_only=bool(pol.get("translation_only", False)) if tok=="aa" else False,
            save_to_disk=True, out_path=enc_path
        )

    last_total = train_on_encoded(
        args.accession, encoded,
        steps=steps, batch_size=batch_size,
        state=state, io_cfg=io_cfg, train_cfg=train_cfg,
        tokenizer=tok, window_size_bp=window_size,
        loss_type=getattr(args, "loss_type", None),
        mask_prob=pol.get("mask_prob"),
        span_mask_prob=pol.get("span_mask_prob"),
        span_mask_len=pol.get("span_mask_len"),
    )

    pvc = state["plasmid_visit_counts"]
    pvc[args.accession] = pvc.get(args.accession, 0) + 1
    save_state(io_cfg.state_file, state)

    print(f"{args.accession}: train-one tokenizer={tok} source={src} steps={steps} batch={batch_size} last_total={last_total:.6f}")
    return 0


def cmd_scope_one(args: argparse.Namespace) -> int:
    if curses is None:
        raise RuntimeError("curses not available")
    cfg = load_full_config(args.config)
    ncbi_cfg, train_cfg, io_cfg = extract_configs(cfg)
    ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)

    tok = _get_tok(args, train_cfg)
    frame = _get_frame(args, train_cfg)
    min_orf = _get_min_orf(args, train_cfg)
    window_size, stride = _pick_window_stride(args, train_cfg, tok)
    _validate_tok_params(tok, window_size, stride, frame)

    src = _get_source(args, tok)
    _ensure_record(args.accession, src, io_cfg=io_cfg, ncbi_cfg=ncbi_cfg, force=False)

    enc_path = encoded_cache_path(
        io_cfg, args.accession, tok, window_size, stride, frame,
        source=src,
        min_orf_aa=(min_orf if tok=="aa" else None),
    )
    if os.path.exists(enc_path) and not args.reencode:
        encoded = np.load(enc_path)
    else:
        encoded = encode_accession(
            args.accession, io_cfg, window_size, stride,
            tokenizer=tok, frame_offset=frame, min_orf_aa=min_orf,
            source=src,
            save_to_disk=True, out_path=enc_path
        )

    errors = compute_window_errors(
        args.accession,
        encoded,
        io_cfg=io_cfg,
        train_cfg=train_cfg,
        tokenizer=tok,
        window_size_bp=window_size,
        loss_type=getattr(args, "loss_type", None),
    )
    metric = compute_gc_from_encoded(encoded, tokenizer=tok)

    curses.wrapper(
        run_scope_ui,
        accession=args.accession,
        errors=errors,
        gc_values=metric,
        window_size=window_size,
        stride=stride,
        fps=args.fps,
    )
    return 0


def cmd_scope_stream(args: argparse.Namespace) -> int:
    if curses is None:
        raise RuntimeError("curses not available")
    cfg = load_full_config(args.config)
    ncbi_cfg, train_cfg, io_cfg = extract_configs(cfg)
    ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)

    tok = _get_tok(args, train_cfg)
    frame = _get_frame(args, train_cfg)
    min_orf = _get_min_orf(args, train_cfg)
    window_size, stride = _pick_window_stride(args, train_cfg, tok)
    _validate_tok_params(tok, window_size, stride, frame)

    src = _get_source(args, tok)
    _ensure_record(args.accession, src, io_cfg=io_cfg, ncbi_cfg=ncbi_cfg, force=False)

    steps = args.steps or train_cfg.steps_per_plasmid
    batch_size = args.batch_size or train_cfg.batch_size

    enc_path = encoded_cache_path(
        io_cfg, args.accession, tok, window_size, stride, frame,
        source=src,
        min_orf_aa=(min_orf if tok=="aa" else None),
    )
    if os.path.exists(enc_path) and not args.reencode:
        encoded = np.load(enc_path)
    else:
        encoded = encode_accession(
            args.accession, io_cfg, window_size, stride,
            tokenizer=tok, frame_offset=frame, min_orf_aa=min_orf,
            source=src,
            save_to_disk=True, out_path=enc_path
        )

    metric = compute_gc_from_encoded(encoded, tokenizer=tok)

    import torch
    from torch.utils.data import DataLoader, TensorDataset
    from .model import get_device, load_or_init_model
    from .encoding import tokenizer_meta

    device = get_device()
    seq_len, vocab_size = tokenizer_meta(tok, window_size)
    hidden_dim = train_cfg.hidden_dim

    lt = (args.loss_type if getattr(args, "loss_type", None) is not None else ("ce" if tok == "aa" else "mse"))

    model, optimizer, global_step, ckpt_path = load_or_init_model(
        io_cfg=io_cfg, seq_len=seq_len, vocab_size=vocab_size,
        hidden_dim=hidden_dim, learning_rate=train_cfg.learning_rate,
        device=device, tokenizer=tok, loss_type=lt
    )

    windows_tensor = torch.from_numpy(encoded)
    dataset = TensorDataset(windows_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)

    ctx = ScopeStreamContext(
        model=model, optimizer=optimizer, device=device,
        dataloader=dataloader, dataloader_iter=iter(dataloader),
        global_step=global_step, last_total=0.0,
        steps_target=steps, steps_done=0,
        beta_kl=train_cfg.beta_kl, kl_warmup_steps=train_cfg.kl_warmup_steps,
        max_grad_norm=train_cfg.max_grad_norm,
        loss_type=lt, seq_len=int(seq_len), vocab_size=int(vocab_size),
    )

    curses.wrapper(
        run_scope_stream_ui,
        accession=args.accession,
        windows_tensor=windows_tensor,
        gc_values=metric,
        window_size=window_size,
        stride=stride,
        fps=args.fps,
        update_every=args.update_every,
        ctx=ctx,
    )
    return 0


def cmd_stream(args: argparse.Namespace) -> int:
    import random
    cfg = load_full_config(args.config)
    ncbi_cfg, train_cfg, io_cfg = extract_configs(cfg)
    ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)

    accessions = read_catalog(args.catalog)
    state = load_state(io_cfg.state_file)

    tok = _get_tok(args, train_cfg)
    frame = _get_frame(args, train_cfg)
    min_orf = _get_min_orf(args, train_cfg)
    window_size, stride = _pick_window_stride(args, train_cfg, tok)
    _validate_tok_params(tok, window_size, stride, frame)

    src = _get_source(args, tok)

    batch_size = args.batch_size or train_cfg.batch_size
    steps_per_plasmid = args.steps_per_plasmid or train_cfg.steps_per_plasmid
    max_epochs = args.max_epochs or train_cfg.max_stream_epochs

    epoch = int(state.get("epoch", 0))

    while epoch < max_epochs:
        indices = list(range(len(accessions)))
        if train_cfg.shuffle_catalog:
            random.shuffle(indices)

        for idx in indices:
            acc = accessions[idx]
            pol = _resolve_proteome_params(args, train_cfg, state=state, tok=tok, src=src)

            _ensure_record(acc, src, io_cfg=io_cfg, ncbi_cfg=ncbi_cfg, force=False)

            enc_path = encoded_cache_path(
                io_cfg, acc, tok, window_size, stride, frame,
                source=src,
                min_orf_aa=(min_orf if tok=="aa" else None),
                max_windows_per_protein=(pol.get("max_windows_per_protein") if tok=="aa" else None),
                protein_len_min=(pol.get("protein_len_min") if tok=="aa" else None),
                protein_len_max=(pol.get("protein_len_max") if tok=="aa" else None),
                translation_only=bool(pol.get("translation_only", False)) if tok=="aa" else False,
                curriculum_tag=pol.get("curriculum_tag"),
            )
            if os.path.exists(enc_path):
                encoded = np.load(enc_path)
            else:
                encoded = encode_accession(
                    acc, io_cfg, window_size, stride,
                    tokenizer=tok, frame_offset=frame, min_orf_aa=min_orf,
                    source=src,
                    max_windows_per_protein=(pol.get("max_windows_per_protein") if tok=="aa" else None),
                    protein_len_min=(pol.get("protein_len_min") if tok=="aa" else None),
                    protein_len_max=(pol.get("protein_len_max") if tok=="aa" else None),
                    translation_only=bool(pol.get("translation_only", False)) if tok=="aa" else False,
                    save_to_disk=True, out_path=enc_path
                )

            _ = train_on_encoded(
                acc, encoded,
                steps=steps_per_plasmid, batch_size=batch_size,
                state=state, io_cfg=io_cfg, train_cfg=train_cfg,
                tokenizer=tok, window_size_bp=window_size,
                loss_type=getattr(args, "loss_type", None),
                mask_prob=pol.get("mask_prob"),
                span_mask_prob=pol.get("span_mask_prob"),
                span_mask_len=pol.get("span_mask_len"),
            )

            pvc = state["plasmid_visit_counts"]
            pvc[acc] = pvc.get(acc, 0) + 1
            state["current_index"] = idx
            state["epoch"] = epoch
            save_state(io_cfg.state_file, state)

            if args.delete_cache:
                cleanup_accession_files(acc, io_cfg, enc_path)

        epoch += 1

    print("[stream] Training complete.")
    return 0

def cmd_generate_plasmid(args: argparse.Namespace) -> int:
    cfg = load_full_config(args.config)
    _, train_cfg, io_cfg = extract_configs(cfg)
    ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)

    tok = _get_tok(args, train_cfg)
    if tok not in ("base", "codon"):
        raise ValueError("generate-plasmid supports tokenizer base|codon only (use generate-protein for aa).")

    window_size = args.window_size if args.window_size is not None else train_cfg.window_size
    stride = train_cfg.stride
    frame = _get_frame(args, train_cfg)
    _validate_tok_params(tok, int(window_size), int(stride), frame)

    seq = generate_plasmid_sequence(
        train_cfg=train_cfg,
        io_cfg=io_cfg,
        length_bp=args.length_bp,
        num_windows=args.num_windows,
        window_size_bp=int(window_size),
        seed=args.seed,
        latent_scale=args.latent_scale,
        temperature=args.temperature,
        gc_bias=args.gc_bias,
        name=args.name,
        output_path=args.output,
        tokenizer=tok,
    )
    print(f"[generate-plasmid] tokenizer={tok} wrote {len(seq)} bp -> {args.output}")
    return 0

def cmd_generate_protein(args: argparse.Namespace) -> int:
    cfg = load_full_config(args.config)
    _, train_cfg, io_cfg = extract_configs(cfg)
    ensure_dirs(io_cfg); setup_logging(io_cfg.logs_dir)

    tok = "aa"
    window_aa = args.window_aa if args.window_aa is not None else train_cfg.protein_window_aa

    seq = generate_protein_sequence(
        train_cfg=train_cfg,
        io_cfg=io_cfg,
        length_aa=args.length_aa,
        num_windows=args.num_windows,
        window_aa=int(window_aa),
        seed=args.seed,
        latent_scale=args.latent_scale,
        temperature=args.temperature,
        name=args.name,
        output_path=args.output,
        reject=bool(getattr(args, "reject", False)),
        reject_tries=int(getattr(args, "reject_tries", 40)),
        reject_max_run=int(getattr(args, "reject_max_run", 10)),
        reject_max_x_frac=float(getattr(args, "reject_max_x_frac", 0.15)),
    )
    print(f"[generate-protein] wrote {len(seq)} aa -> {args.output}")
    return 0

def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="stream_train.py", description="Genostream streaming VAE trainer + scope.")
    p.add_argument("--config", default="stream_config.yaml", help="YAML config path (default: stream_config.yaml)")
    sub = p.add_subparsers(dest="command", required=True)

    s = sub.add_parser("init"); s.set_defaults(func=cmd_init)
    s = sub.add_parser("catalog-show"); s.add_argument("path"); s.set_defaults(func=cmd_catalog_show)

    s = sub.add_parser("fetch-one")
    s.add_argument("accession"); s.add_argument("--force", action="store_true")
    s.add_argument("--source", choices=["fasta","genbank"], default=None, help="Fetch record source (default: fasta)")
    s.set_defaults(func=cmd_fetch_one)

    def add_tok_args(sp):
        sp.add_argument("--tokenizer", choices=["base","codon","aa"], default=None, help="Override tokenizer (default from config)")
        sp.add_argument("--frame-offset", type=int, choices=[0,1,2], default=None, help="Codon frame offset (default from config)")
        sp.add_argument("--min-orf-aa", type=int, default=None, help="AA tokenizer: minimum ORF length in amino acids (default from config)")
        sp.add_argument("--source", choices=["fasta","genbank"], default=None, help="Sequence record source. Default: fasta for base/codon, genbank for aa.")
        # Proteome (aa) sampling / filters
        sp.add_argument("--protein-len-min", type=int, default=None, help="AA tokenizer: minimum protein length to include (overrides config/curriculum)")
        sp.add_argument("--protein-len-max", type=int, default=None, help="AA tokenizer: maximum protein length to include (overrides config/curriculum)")
        sp.add_argument("--max-windows-per-protein", type=int, default=None, help="AA tokenizer: cap windows sampled per CDS/protein (balances long proteins)")
        sp.add_argument("--translation-only", action="store_true", default=None, help="AA tokenizer+genbank: use only /translation-provided proteins")
        sp.add_argument("--allow-translated", action="store_true", default=None, help="AA tokenizer+genbank: allow translating from CDS when /translation missing")
        sp.add_argument("--no-curriculum", action="store_true", help="Disable proteome curriculum for this run")

    def add_loss_args(sp):
        sp.add_argument(
            "--loss-type",
            choices=["mse", "ce"],
            default=None,
            help="Override reconstruction loss. Default: ce for aa, mse for base/codon.",
        )
        sp.add_argument(
            "--mask-prob",
            type=float,
            default=None,
            help="Denoising mask probability (AA tokenizer only). Default: 0.05 for aa, 0 for others.",
        )
        sp.add_argument("--span-mask-prob", type=float, default=None, help="AA tokenizer: probability to apply a contiguous span mask per sequence")
        sp.add_argument("--span-mask-len", type=int, default=None, help="AA tokenizer: length (aa) of the contiguous span mask")

    s = sub.add_parser("encode-one")
    s.add_argument("accession")
    s.add_argument("--window-size", type=int, default=None)
    s.add_argument("--stride", type=int, default=None)
    add_tok_args(s)
    s.set_defaults(func=cmd_encode_one)

    s = sub.add_parser("train-one")
    s.add_argument("accession")
    s.add_argument("--steps", type=int, default=None)
    s.add_argument("--batch-size", type=int, default=None)
    s.add_argument("--window-size", type=int, default=None)
    s.add_argument("--stride", type=int, default=None)
    s.add_argument("--reencode", action="store_true")
    add_tok_args(s)
    add_loss_args(s)
    s.set_defaults(func=cmd_train_one)

    s = sub.add_parser("scope-one")
    s.add_argument("accession")
    s.add_argument("--window-size", type=int, default=None)
    s.add_argument("--stride", type=int, default=None)
    s.add_argument("--fps", type=float, default=12.0)
    s.add_argument("--reencode", action="store_true")
    add_tok_args(s)
    s.add_argument("--loss-type", choices=["mse", "ce"], default=None, help="Override loss used for error metric (default: ce for aa, mse for base/codon)")
    s.set_defaults(func=cmd_scope_one)

    s = sub.add_parser("scope-stream")
    s.add_argument("accession")
    s.add_argument("--steps", type=int, default=None)
    s.add_argument("--batch-size", type=int, default=None)
    s.add_argument("--window-size", type=int, default=None)
    s.add_argument("--stride", type=int, default=None)
    s.add_argument("--fps", type=float, default=12.0)
    s.add_argument("--update-every", type=int, default=5)
    s.add_argument("--reencode", action="store_true")
    add_tok_args(s)
    add_loss_args(s)
    s.set_defaults(func=cmd_scope_stream)

    s = sub.add_parser("stream")
    s.add_argument("--catalog", required=True)
    s.add_argument("--max-epochs", type=int, default=None)
    s.add_argument("--steps-per-plasmid", type=int, default=None)
    s.add_argument("--batch-size", type=int, default=None)
    s.add_argument("--window-size", type=int, default=None)
    s.add_argument("--stride", type=int, default=None)
    s.add_argument("--delete-cache", action="store_true")
    add_tok_args(s)
    add_loss_args(s)
    s.set_defaults(func=cmd_stream)

    s = sub.add_parser("generate-plasmid")
    s.add_argument("--length-bp", type=int, default=10000)
    s.add_argument("--num-windows", type=int, default=None)
    s.add_argument("--window-size", type=int, default=None)
    s.add_argument("--name", default="genostream_plasmid_1")
    s.add_argument("--output", default="generated/novel_plasmid.fasta")
    s.add_argument("--seed", type=int, default=None)
    s.add_argument("--latent-scale", type=float, default=1.0)
    s.add_argument("--temperature", type=float, default=1.0)
    s.add_argument("--gc-bias", type=float, default=1.0)
    add_tok_args(s)
    s.set_defaults(func=cmd_generate_plasmid)

    s = sub.add_parser("generate-protein")
    s.add_argument("--length-aa", type=int, default=600)
    s.add_argument("--num-windows", type=int, default=None)
    s.add_argument("--window-aa", type=int, default=None)
    s.add_argument("--name", default="genostream_protein_1")
    s.add_argument("--output", default="generated/novel_protein.faa")
    s.add_argument("--seed", type=int, default=None)
    s.add_argument("--latent-scale", type=float, default=1.0)
    s.add_argument("--temperature", type=float, default=1.0)
    s.add_argument("--reject", action="store_true", help="Rejection-sample until a basic protein-like filter passes")
    s.add_argument("--reject-tries", type=int, default=40)
    s.add_argument("--reject-max-run", type=int, default=10, help="Reject if any AA repeats longer than this")
    s.add_argument("--reject-max-x-frac", type=float, default=0.15, help="Reject if fraction of 'X' exceeds this")
    s.set_defaults(func=cmd_generate_protein)

    return p

def main(argv: Any = None) -> int:
    args = build_parser().parse_args(argv)
    return args.func(args)

if __name__ == "__main__":
    raise SystemExit(main())
